{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras import Model, layers\n",
    "import csv\n",
    "import re\n",
    "import pylab\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "HIDDEN_SIZE = 20\n",
    "EMBEDDING_SIZE = 20\n",
    "MAX_LABEL = 15\n",
    "\n",
    "batch_size = 128\n",
    "no_epochs = 250\n",
    "lr = 0.01\n",
    "\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_dict(contents):\n",
    "    words = list()\n",
    "    for content in contents:\n",
    "        for word in word_tokenize(clean_str(content)):\n",
    "            words.append(word)\n",
    "\n",
    "    word_counter = collections.Counter(words).most_common()\n",
    "    word_dict = dict()\n",
    "    word_dict[\"<pad>\"] = 0\n",
    "    word_dict[\"<unk>\"] = 1\n",
    "    word_dict[\"<eos>\"] = 2\n",
    "    for word, _ in word_counter:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(contents, word_dict, document_max_len):\n",
    "    x = list(map(lambda d: word_tokenize(clean_str(d)), contents))\n",
    "    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
    "    x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n",
    "    x = list(map(lambda d: d[:document_max_len], x))\n",
    "    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_words():\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
    "    with open('./train_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_train.append(data)\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open('./test_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_test.append(data)\n",
    "            y_test.append(int(row[0]))\n",
    "\n",
    "    word_dict = build_word_dict(x_train+x_test)\n",
    "    x_train = preprocess_word(x_train, word_dict, MAX_DOCUMENT_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "    x_test = preprocess_word(x_test, word_dict, MAX_DOCUMENT_LENGTH)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    x_train = [x[:MAX_DOCUMENT_LENGTH] for x in x_train]\n",
    "    x_test = [x[:MAX_DOCUMENT_LENGTH] for x in x_test]\n",
    "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
    "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
    "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
    "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
    "\n",
    "    vocab_size = tf.get_static_value(tf.reduce_max(x_train))\n",
    "    vocab_size = max(vocab_size, tf.get_static_value(tf.reduce_max(x_test))) + 1\n",
    "    return x_train, y_train, x_test, y_test, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "class WordRNN(Model):\n",
    "    def __init__(self, vocab_size, hidden_dim=20):\n",
    "        super(WordRNN, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = layers.Embedding(vocab_size, 20, input_length=MAX_DOCUMENT_LENGTH)\n",
    "        \n",
    "        # Weight variables and RNN cell\n",
    "        self.rnn = layers.LSTM(self.hidden_dim, unroll=True)\n",
    "        self.dense = layers.Dense(MAX_LABEL, activation=None)\n",
    "\n",
    "    def call(self, x, drop_rate):\n",
    "        # forward logic\n",
    "        embedding = self.embedding(x)\n",
    "        encoding = self.rnn(embedding)\n",
    "\n",
    "        encoding = tf.nn.dropout(encoding, drop_rate)\n",
    "        logits = self.dense(encoding)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_step(model, x, label, drop_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(x, drop_rate)\n",
    "        loss = loss_object(label, out)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test_step(model, x, label, drop_rate=0):\n",
    "    out = model(x,drop_rate)\n",
    "    t_loss = loss_object(label, out)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(label, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.6668291091918945, Accuracy: 0.07214285433292389, Test Loss: 2.6453518867492676, Test Accuracy: 0.0714285746216774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-5988a416100b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-df9da8eaf955>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, x, label, drop_rate)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_aggregate_grads\u001b[1;34m(gradients)\u001b[0m\n\u001b[0;32m    628\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_n\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m     assert all(isinstance(g, (ops.Tensor, ops.IndexedSlices))\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36madd_n\u001b[1;34m(inputs, name)\u001b[0m\n\u001b[0;32m    401\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m    402\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"AddN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 403\u001b[1;33m         inputs)\n\u001b[0m\u001b[0;32m    404\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fig_dir = \"./q6a_figure\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.mkdir(fig_dir)\n",
    "\n",
    "#String for checking and Title for images\n",
    "#drop = \"with_drop\"\n",
    "drop = \"no_dropout\"\n",
    "\n",
    "arch = \"LSTM\"\n",
    "\n",
    "x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n",
    "# Use `tf.data` to batch and shuffle the dataset:\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "model = WordRNN(vocab_size, HIDDEN_SIZE)\n",
    "\n",
    "# Choose optimizer and loss function for training\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Select metrics to measure the loss and the accuracy of the model. \n",
    "# These metrics accumulate the values over epochs and then print the overall result.\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "train_acc = []\n",
    "train_cost = []\n",
    "test_acc = []\n",
    "test_cost = []\n",
    "for epoch in range(no_epochs):\n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    if drop == \"with drop\":\n",
    "        for images, labels in train_ds:\n",
    "            train_step(model, images, labels, drop_rate=0.5)\n",
    "    else:\n",
    "        for images, labels in train_ds:\n",
    "            train_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "    for images, labels in test_ds:\n",
    "        test_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "    train_acc.append(train_accuracy.result())\n",
    "    train_cost.append(train_loss.result())\n",
    "    test_acc.append(test_accuracy.result())\n",
    "    test_cost.append(test_loss.result())\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print (template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result(),\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()))\n",
    "\n",
    "# Plot test accuracy\n",
    "pylab.figure()\n",
    "pylab.plot(test_acc, label='Test')\n",
    "pylab.plot(train_acc, label='Train')\n",
    "pylab.xlabel('epochs')\n",
    "pylab.ylabel(f'WordRNN {arch}_{drop} Accuracy')\n",
    "pylab.legend(loc='lower right')\n",
    "pylab.savefig(f\"{fig_dir}/WordRNN_{arch}_{drop}_accuracy.png\")\n",
    "pylab.close()\n",
    "\n",
    "# Plot train error\n",
    "pylab.figure()\n",
    "pylab.plot(train_cost, label='Train')\n",
    "pylab.plot(test_cost, label='Test')\n",
    "pylab.xlabel('epochs')\n",
    "pylab.ylabel(f'WordRNN {arch}_{drop} Loss')\n",
    "pylab.legend(loc='lower right')\n",
    "pylab.savefig(f\"{fig_dir}/WordRNN_{arch}_{drop}_loss.png\")\n",
    "pylab.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
