{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import collections\n",
    "import tensorflow as tf\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tensorflow.keras import Model, layers\n",
    "import csv\n",
    "import re\n",
    "import pylab\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data with [character]\n",
    "def vocabulary(strings):\n",
    "    chars = sorted(list(set(list(''.join(strings)))))\n",
    "    char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "    vocab_size = len(chars)\n",
    "    return vocab_size, char_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_char(strings, char_to_ix, MAX_LENGTH):\n",
    "    data_chars = [list(d.lower()) for _, d in enumerate(strings)]\n",
    "    for i, d in enumerate(data_chars):\n",
    "        if len(d)>MAX_LENGTH:\n",
    "            d = d[:MAX_LENGTH]\n",
    "        elif len(d) < MAX_LENGTH:\n",
    "            d += [' '] * (MAX_LENGTH - len(d))\n",
    "            \n",
    "    data_ids = np.zeros([len(data_chars), MAX_LENGTH], dtype=np.int64)\n",
    "    for i in range(len(data_chars)):\n",
    "        for j in range(MAX_LENGTH):\n",
    "            data_ids[i, j] = char_to_ix[data_chars[i][j]]\n",
    "    return np.array(data_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_chars():\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
    "    with open('./train_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_train.append(data)\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open('./test_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_test.append(data)\n",
    "            y_test.append(int(row[0]))\n",
    "\n",
    "\n",
    "    vocab_size, char_to_ix = vocabulary(x_train+x_test)\n",
    "    x_train = preprocess_char(x_train, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "    x_test = preprocess_char(x_test, char_to_ix, MAX_DOCUMENT_LENGTH)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
    "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
    "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
    "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function for Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "    text = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`\\\"]\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.strip().lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_word_dict(contents):\n",
    "    words = list()\n",
    "    for content in contents:\n",
    "        for word in word_tokenize(clean_str(content)):\n",
    "            words.append(word)\n",
    "\n",
    "    word_counter = collections.Counter(words).most_common()\n",
    "    word_dict = dict()\n",
    "    word_dict[\"<pad>\"] = 0\n",
    "    word_dict[\"<unk>\"] = 1\n",
    "    word_dict[\"<eos>\"] = 2\n",
    "    for word, _ in word_counter:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    return word_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_word(contents, word_dict, document_max_len):\n",
    "    x = list(map(lambda d: word_tokenize(clean_str(d)), contents))\n",
    "    x = list(map(lambda d: list(map(lambda w: word_dict.get(w, word_dict[\"<unk>\"]), d)), x))\n",
    "    x = list(map(lambda d: d + [word_dict[\"<eos>\"]], x))\n",
    "    x = list(map(lambda d: d[:document_max_len], x))\n",
    "    x = list(map(lambda d: d + (document_max_len - len(d)) * [word_dict[\"<pad>\"]], x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_words():\n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "    cop = re.compile(\"[^a-z^A-Z^0-9^,^.^' ']\")\n",
    "    with open('./train_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_train.append(data)\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open('./test_medium.csv', encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            data = cop.sub(\"\", row[1])\n",
    "            x_test.append(data)\n",
    "            y_test.append(int(row[0]))\n",
    "\n",
    "    word_dict = build_word_dict(x_train+x_test)\n",
    "    x_train = preprocess_word(x_train, word_dict, MAX_DOCUMENT_LENGTH)\n",
    "    y_train = np.array(y_train)\n",
    "    x_test = preprocess_word(x_test, word_dict, MAX_DOCUMENT_LENGTH)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    x_train = [x[:MAX_DOCUMENT_LENGTH] for x in x_train]\n",
    "    x_test = [x[:MAX_DOCUMENT_LENGTH] for x in x_test]\n",
    "    x_train = tf.constant(x_train, dtype=tf.int64)\n",
    "    y_train = tf.constant(y_train, dtype=tf.int64)\n",
    "    x_test = tf.constant(x_test, dtype=tf.int64)\n",
    "    y_test = tf.constant(y_test, dtype=tf.int64)\n",
    "\n",
    "    vocab_size = tf.get_static_value(tf.reduce_max(x_train))\n",
    "    vocab_size = max(vocab_size, tf.get_static_value(tf.reduce_max(x_test))) + 1\n",
    "    return x_train, y_train, x_test, y_test, vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SHAPE2 = [20, 1]\n",
    "N_FILTERS = 10\n",
    "MAX_DOCUMENT_LENGTH = 100\n",
    "one_hot_size = 256\n",
    "HIDDEN_SIZE = 20\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "MAX_LABEL = 15\n",
    "EMBEDDING_SIZE = 20\n",
    "\n",
    "batch_size = 128\n",
    "no_epochs = 2\n",
    "lr = 0.01\n",
    "\n",
    "seed = 10\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "class CharCNN(Model):\n",
    "    def __init__(self, vocab_size=256):\n",
    "        super(CharCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Weight variables and CNN cell\n",
    "        self.conv1 = layers.Conv2D(N_FILTERS, [20,256], padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool1 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.conv2 = layers.Conv2D(N_FILTERS, FILTER_SHAPE2, padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool2 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
    "\n",
    "    def call(self, x, drop_rate=0.5):\n",
    "        # forward\n",
    "        x = tf.one_hot(x, one_hot_size)\n",
    "        x = x[..., tf.newaxis] \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = tf.nn.dropout(x, drop_rate)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "class WordCNN(Model):\n",
    "    def __init__(self, vocab_size=20):\n",
    "        super(WordCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = layers.Embedding(vocab_size, EMBEDDING_SIZE, input_length=MAX_DOCUMENT_LENGTH)\n",
    "        \n",
    "        # Weight variables and CNN cell\n",
    "        self.conv1 = layers.Conv2D(N_FILTERS, [20,20], padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool1 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.conv2 = layers.Conv2D(N_FILTERS, FILTER_SHAPE2, padding='VALID', activation='relu', use_bias=True)\n",
    "        self.pool2 = layers.MaxPool2D(POOLING_WINDOW, POOLING_STRIDE, padding='SAME')\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.dense = layers.Dense(MAX_LABEL, activation='softmax')\n",
    "\n",
    "    def call(self, x, drop_rate=0.5):\n",
    "        # forward\n",
    "        x = self.embedding(x)\n",
    "        x = x[..., tf.newaxis] \n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = tf.nn.dropout(x, drop_rate)\n",
    "        logits = self.dense(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "class CharRNN(Model):\n",
    "    def __init__(self, vocab_size=256, hidden_dim=20):\n",
    "        super(CharRNN, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Weight variables and RNN cell\n",
    "        self.rnn = layers.RNN(\n",
    "            tf.keras.layers.GRUCell(self.hidden_dim), unroll=True)\n",
    "        self.dense = layers.Dense(MAX_LABEL, activation=None)\n",
    "\n",
    "    def call(self, x, drop_rate):\n",
    "        # forward logic\n",
    "        x = tf.one_hot(x, one_hot_size)\n",
    "        x = self.rnn(x)\n",
    "        x = tf.nn.dropout(x, drop_rate)\n",
    "        logits = self.dense(x)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "tf.keras.backend.set_floatx('float32')\n",
    "class WordRNN(Model):\n",
    "    def __init__(self, vocab_size, hidden_dim=20):\n",
    "        super(WordRNN, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = layers.Embedding(vocab_size, 20, input_length=MAX_DOCUMENT_LENGTH)\n",
    "        \n",
    "        # Weight variables and RNN cell\n",
    "        self.rnn = layers.RNN(\n",
    "            tf.keras.layers.GRUCell(self.hidden_dim), unroll=True)\n",
    "        self.dense = layers.Dense(MAX_LABEL, activation=None)\n",
    "\n",
    "    def call(self, x, drop_rate):\n",
    "        # forward logic\n",
    "        embedding = self.embedding(x)\n",
    "        encoding = self.rnn(embedding)\n",
    "\n",
    "        encoding = tf.nn.dropout(encoding, drop_rate)\n",
    "        logits = self.dense(encoding)\n",
    "    \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_step(model, x, label, drop_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        out = model(x, drop_rate)\n",
    "        loss = loss_object(label, out)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing function\n",
    "def test_step(model, x, label, drop_rate=0):\n",
    "    out = model(x,drop_rate)\n",
    "    t_loss = loss_object(label, out)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(label, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 Model - CharCNN_no_drop\n",
      "Epoch 1, Loss: 2.6183876991271973, Accuracy: 0.18464285135269165, Test Loss: 2.5028140544891357, Test Accuracy: 0.3257142901420593\n",
      "Epoch 2, Loss: 2.445434331893921, Accuracy: 0.3700000047683716, Test Loss: 2.365260362625122, Test Accuracy: 0.4628571569919586\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-db3f34f15f93>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_ds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-df9da8eaf955>\u001b[0m in \u001b[0;36mtrain_step\u001b[1;34m(model, x, label, drop_rate)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1048\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gradient_tape/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    600\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 602\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    603\u001b[0m   ]\n\u001b[0;32m    604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1079\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1080\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1081\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1082\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fig_dir = \"./all_figure\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.mkdir(fig_dir)\n",
    "\n",
    "acc_file = \"accuracy.xlsx\"\n",
    "\n",
    "question = [1,2,3,4,'5.1','5.2','5.3','5.4'] #Question\n",
    "\n",
    "for model_ in question:\n",
    "    if model_ == 1: #CharCNN drop\n",
    "        x_train, y_train, x_test, y_test, _ = read_data_chars()\n",
    "        model = CharCNN(256)\n",
    "        drop = False\n",
    "        model_name = \"CharCNN_no_drop\"\n",
    "    elif model_ == 2:  #WordCNN drop\n",
    "        x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n",
    "        model = WordCNN(vocab_size)\n",
    "        drop = False\n",
    "        model_name = \"WordCNN_no_drop\"\n",
    "    elif model_ == 3:  #CharRNN drop\n",
    "        x_train, y_train, x_test, y_test, _ = read_data_chars()\n",
    "        model = CharRNN(256, HIDDEN_SIZE)\n",
    "        drop = False\n",
    "        model_name = \"CharRNN_no_drop\"\n",
    "    elif model_ == 4:  #WordRNN drop\n",
    "        x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n",
    "        model = WordRNN(vocab_size, HIDDEN_SIZE)\n",
    "        drop = False\n",
    "        model_name = \"WordRNN_no_drop\"\n",
    "    elif model_ == '5.1':\n",
    "        x_train, y_train, x_test, y_test, _ = read_data_chars()\n",
    "        model = CharCNN(256)\n",
    "        drop = True\n",
    "        model_name = \"CharCNN_w_drop\"\n",
    "    elif model_ == '5.2':  #WordCNN drop\n",
    "        x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n",
    "        model = WordCNN(vocab_size)\n",
    "        drop = True\n",
    "        model_name = \"WordCNN_w_drop\"\n",
    "    elif model_ == '5.3':  #CharRNN drop\n",
    "        x_train, y_train, x_test, y_test, _ = read_data_chars()\n",
    "        model = CharRNN(256, HIDDEN_SIZE)\n",
    "        drop = True\n",
    "        model_name = \"CharRNN_w_drop\"\n",
    "    elif model_ == '5.4':  #WordRNN drop\n",
    "        x_train, y_train, x_test, y_test, vocab_size = read_data_words()\n",
    "        model = WordRNN(vocab_size, HIDDEN_SIZE)\n",
    "        drop = True\n",
    "        model_name = \"WordRNN_w_drop\"\n",
    "    else:\n",
    "        raise NotImplementedError(f'Error on Model If-Else')\n",
    "    \n",
    "    print(f\"Question {model_} Model - {model_name}\")\n",
    "    \n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
    "\n",
    "    # Choose optimizer and loss function for training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Select metrics to measure the loss and the accuracy of the model. \n",
    "    # These metrics accumulate the values over epochs and then print the overall result.\n",
    "    train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "    train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "    test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "    train_acc = []\n",
    "    train_cost = []\n",
    "    test_acc = []\n",
    "    test_cost = []\n",
    "    \n",
    "    for epoch in range(no_epochs):\n",
    "        # Reset the metrics at the start of the next epoch\n",
    "        train_loss.reset_states()\n",
    "        train_accuracy.reset_states()\n",
    "        test_loss.reset_states()\n",
    "        test_accuracy.reset_states()\n",
    "\n",
    "        for images, labels in train_ds:\n",
    "            if drop == True:\n",
    "                train_step(model, images, labels, drop_rate=0.5)\n",
    "            else:\n",
    "                train_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "        for images, labels in test_ds:\n",
    "            test_step(model, images, labels, drop_rate=0)\n",
    "\n",
    "        train_acc.append(train_accuracy.result())\n",
    "        train_cost.append(train_loss.result())\n",
    "        test_acc.append(test_accuracy.result())\n",
    "        test_cost.append(test_loss.result())\n",
    "        template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "        print (template.format(epoch+1,\n",
    "                              train_loss.result(),\n",
    "                              train_accuracy.result(),\n",
    "                              test_loss.result(),\n",
    "                              test_accuracy.result()))\n",
    "    \n",
    "    pylab.figure()\n",
    "    pylab.plot(test_acc, label='Test')\n",
    "    pylab.plot(train_acc, label='Train')\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('{model_name} Accuracy')\n",
    "    pylab.legend(loc='lower right')\n",
    "    pylab.savefig(f\"{fig_dir}/q{model_}_{model_name}_Accuracy.png\")\n",
    "    pylab.close()\n",
    "\n",
    "    # Plot train error\n",
    "    pylab.figure()\n",
    "    pylab.plot(train_cost, label='Train')\n",
    "    pylab.plot(test_cost, label='Test')\n",
    "    pylab.xlabel('epochs')\n",
    "    pylab.ylabel('{model_name} Loss')\n",
    "    pylab.legend(loc='lower right')\n",
    "    pylab.savefig(f\"{fig_dir}/q{model_}_{model_name}_loss.png\")\n",
    "    pylab.close()\n",
    "\n",
    "    if os.path.exists(acc_file):\n",
    "        df = pd.read_excel(acc_file)\n",
    "        df[model_name+\"_test_acc\"] = np.array(test_acc)\n",
    "    else:\n",
    "        df = pd.DataFrame(np.array(test_acc), columns=[model_name+\"_test_acc\"])\n",
    "    df.to_excel(acc_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
